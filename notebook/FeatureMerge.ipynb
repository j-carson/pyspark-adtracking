{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Merge\n",
    "\n",
    "Loads all the engineered feature tables, merges them into the datasets, and saves new datasets \n",
    "that can go straight to a spark mlib model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x115488a58>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Code to load and merge the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'device', 'os', 'channel', 'app' ]\n",
    "bigrams = [ list(b) for b in combinations(columns,2)]\n",
    "\n",
    "for ftype in [ 'df_', 'tgt_']:\n",
    "    for c in columns:\n",
    "        tmp = spark.read.parquet(f'../data/features/{ftype}{c}_pct.parquet')\n",
    "        tmp.createOrReplaceTempView(f'{ftype}{c}_pct')\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        b = '_'.join(bigram)\n",
    "        tmp = spark.read.parquet(f'../data/features/{ftype}{b}.parquet')\n",
    "        tmp.createOrReplaceTempView(f'{ftype}{b}')\n",
    "        \n",
    "tmp = spark.read.parquet('../data/features/ip_pct.parquet')\n",
    "tmp.createOrReplaceTempView('ip_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The big-hairy SQL method is faster than merging one feature at a time because it gets\n",
    "# optimized under the covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_hairy_sql = \"\"\"\n",
    "    SELECT {tname}.*, \n",
    "           df_device_pct.device_pct,\n",
    "           df_os_pct.os_pct,\n",
    "           df_channel_pct.channel_pct,\n",
    "           df_app_pct.app_pct,\n",
    "           tgt_device_pct.device_pct as tgt_device_pct,\n",
    "           tgt_os_pct.os_pct as tgt_os_pct,\n",
    "           tgt_channel_pct.channel_pct as tgt_channel_pct,\n",
    "           tgt_app_pct.app_pct as tgt_app_pct,\n",
    "           ip_pct.ip_pct,\n",
    "           df_device_app.device_app, \n",
    "           df_device_channel.device_channel,\n",
    "           df_device_os.device_os,\n",
    "           df_os_app.os_app,\n",
    "           df_os_channel.os_channel,\n",
    "           df_channel_app.channel_app,\n",
    "           tgt_device_app.device_app as tgt_device_app,\n",
    "           tgt_device_channel.device_channel as tgt_device_channel,\n",
    "           tgt_device_os.device_os as tgt_device_os,\n",
    "           tgt_os_app.os_app as tgt_os_app,\n",
    "           tgt_os_channel.os_channel as tgt_os_channel,\n",
    "           tgt_channel_app.channel_app as tgt_channel_app\n",
    "    FROM\n",
    "           {tname}\n",
    "    LEFT JOIN df_device_pct      ON {tname}.device = df_device_pct.device\n",
    "    LEFT JOIN df_os_pct          ON {tname}.os = df_os_pct.os\n",
    "    LEFT JOIN df_channel_pct     ON {tname}.channel = df_channel_pct.channel\n",
    "    LEFT JOIN df_app_pct         ON {tname}.app = df_app_pct.app\n",
    "    LEFT JOIN tgt_device_pct     ON {tname}.device = tgt_device_pct.device\n",
    "    LEFT JOIN tgt_os_pct         ON {tname}.os = tgt_os_pct.os\n",
    "    LEFT JOIN tgt_channel_pct    ON {tname}.channel = tgt_channel_pct.channel\n",
    "    LEFT JOIN tgt_app_pct        ON {tname}.app = tgt_app_pct.app\n",
    "    LEFT JOIN ip_pct             ON {tname}.ip = ip_pct.ip AND {tname}.doy = ip_pct.doy\n",
    "    LEFT JOIN df_device_app      ON {tname}.device = df_device_app.device AND {tname}.app = df_device_app.app\n",
    "    LEFT JOIN df_device_channel  ON {tname}.device = df_device_channel.device AND {tname}.channel = df_device_channel.channel\n",
    "    LEFT JOIN df_device_os       ON {tname}.device = df_device_os.device AND {tname}.os = df_device_os.os\n",
    "    LEFT JOIN df_os_app          ON {tname}.os = df_os_app.os AND {tname}.app = df_os_app.app\n",
    "    LEFT JOIN df_os_channel      ON {tname}.os = df_os_channel.os AND {tname}.channel = df_os_channel.channel\n",
    "    LEFT JOIN df_channel_app     ON {tname}.channel = df_channel_app.channel AND {tname}.app = df_channel_app.app\n",
    "    LEFT JOIN tgt_device_app     ON {tname}.device = tgt_device_app.device AND {tname}.app = tgt_device_app.app\n",
    "    LEFT JOIN tgt_device_channel ON {tname}.device = tgt_device_channel.device AND {tname}.channel = tgt_device_channel.channel\n",
    "    LEFT JOIN tgt_device_os      ON {tname}.device = tgt_device_os.device AND {tname}.os = tgt_device_os.os\n",
    "    LEFT JOIN tgt_os_app         ON {tname}.os = tgt_os_app.os AND {tname}.app = tgt_os_app.app\n",
    "    LEFT JOIN tgt_os_channel     ON {tname}.os = tgt_os_channel.os AND {tname}.channel = tgt_os_channel.channel\n",
    "    LEFT JOIN tgt_channel_app    ON {tname}.channel = tgt_channel_app.channel AND {tname}.app = tgt_channel_app.app\n",
    "           \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(sdf, table_name='dataset'):\n",
    "    sdf = sdf.withColumn('doy', F.dayofyear('click_time'))\n",
    "    sdf.createOrReplaceTempView(table_name)\n",
    "    merged = spark.sql(big_hairy_sql.format(tname=table_name))\n",
    "    return merged.fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, merge, and resave each of the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata = spark.read.parquet('../data/intermed/train.parquet')\n",
    "feats = add_features(fulldata)\n",
    "feats.write.parquet('../data/model/trainf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = spark.read.parquet('../data/intermed/train0_10pct.parquet')\n",
    "feats = add_features(class0)\n",
    "feats.write.parquet('../data/model/train0_10pctf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = spark.read.parquet('../data/intermed/train1.parquet')\n",
    "feats = add_features(class1)\n",
    "feats.write.parquet('../data/model/train1f.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = spark.read.parquet('../data/intermed/train_sample.parquet')\n",
    "feats = add_features(mini)\n",
    "feats.write.parquet('../data/model/train_samplef.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.parquet('../data/intermed/test.parquet')\n",
    "feats = add_features(test)\n",
    "feats.write.parquet('../data/model/testf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5572cac05c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# barrier so I don't hit return too many times and kill my spark session :-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# barrier so I don't hit return too many times and kill my spark session :-)\n",
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

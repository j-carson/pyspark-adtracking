{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "I need to create some features for this data. There are too many categories to make one-hot encoded columns for each one. I was playing around with various frequency-related measures to do instead. I'm counting the occurrence of features and pairs of features, for \n",
    "both the dataset as a whole and for class one (target encoding technique).\n",
    "\n",
    "Dividing the count by the number of rows results in tiny values at the tail. Dividing by the maximum count should be more \n",
    "numerically stable.\n",
    "\n",
    "# To do\n",
    "\n",
    "Right now I'm creating feature counts over the entire set of training data (except for ip, which is per calendar day). Data goes \n",
    "stale over time. It would be interesting to see whether the most useful features come from the last hour, 8 hours, day, 2 days, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x103a9ca90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('../data/intermed/train.parquet')\n",
    "class1 = spark.read.parquet('../data/intermed/train1.parquet')\n",
    "test = spark.read.parquet('../data/intermed/test_supplement.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting \n",
    "\n",
    "Each count table gets its own parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_table( sdf, groupby_clause, prefix ):\n",
    "    \n",
    "    if type(groupby_clause) == str:\n",
    "        column_name = groupby_clause + '_pct' # for example: ip_pct\n",
    "        join_clause = [groupby_clause]\n",
    "    else:\n",
    "        column_name = \"_\".join(groupby_clause)  # for example: device_os\n",
    "        join_clause = groupby_clause\n",
    "        \n",
    "    file_name = ''.join([ prefix,column_name,'.parquet'])\n",
    "\n",
    "    counts_sdf =  sdf.groupby( \n",
    "                        groupby_clause \n",
    "                ).count(\n",
    "                ).orderBy(\n",
    "                    'count', ascending = False\n",
    "                )\n",
    "    \n",
    "    maxcnt = counts_sdf.select(F.max('count').alias('maxcnt')).collect()\n",
    "    maxcnt = maxcnt[0].maxcnt\n",
    "    \n",
    "    counts_sdf = counts_sdf.withColumn('ratios',\n",
    "                    F.col('count').astype(T.DoubleType())/float(maxcnt))\n",
    "    counts_sdf = counts_sdf.drop('count').withColumnRenamed('ratios', column_name)\n",
    "    \n",
    "    counts_sdf.write.parquet('../data/features/' + file_name)\n",
    "    return counts_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device, OS, channel, app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not doing ip address yet -- it's special because IP's come and go more quickly over time\n",
    "columns = [ 'device', 'os', 'channel', 'app' ]\n",
    "bigrams = [ list(b) for b in combinations(columns,2)]\n",
    "\n",
    "for c in columns:\n",
    "    make_count_table( df, c, 'df_' )\n",
    "    make_count_table( class1, c, 'tgt_')\n",
    "    \n",
    "for bigram in bigrams:\n",
    "    make_count_table( df, bigram, 'df_' )\n",
    "    make_count_table( class1, bigram, 'tgt_')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip = df.select('ip', 'click_time')\n",
    "test_ip = test.select('ip', 'click_time')\n",
    "ip_data = train_ip.unionAll(test_ip)\n",
    "\n",
    "ip_data = ip_data.withColumn('doy', F.dayofyear('click_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times an ip appears each day\n",
    "day_counts = ip_data.groupby(['doy','ip']).count()\n",
    "# find the max count for each day\n",
    "day_max = day_counts[['doy','count']]\\\n",
    "                .groupby(['doy'])\\\n",
    "                .max()\\\n",
    "                .withColumnRenamed('max(count)', 'day_max')\\\n",
    "                .drop('max(doy)')\n",
    "# merge the max per day into the daily counts table\n",
    "merge = day_counts.join(day_max, ['doy'], how='left')\n",
    "# normalize all the counts by the max\n",
    "ip_table = merge.withColumn('ip_pct',\n",
    "                 F.col('count').astype(T.FloatType())/\n",
    "                 F.col('day_max').astype(T.FloatType())\n",
    "                ).drop(\n",
    "                    'count'\n",
    "                ).drop(\n",
    "                    'day_max'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_table.write.parquet('../data/features/ip_pct.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barrier so I don't accidently kill my spark session by hitting return too many times\n",
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "Datasets from Kaggle are pre-cleaned, but CSV is not the fasted way to read them into future notebooks.\n",
    "\n",
    "This notebook reads the text files, converts the data to the smallest applicable data typesÂ and writes them\n",
    "out as parquet, which is both pandas and pyspark friendly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'int32',\n",
    "        'app'           : 'int16',\n",
    "        'device'        : 'int16',\n",
    "        'os'            : 'int16',\n",
    "        'channel'       : 'int16',\n",
    "        'is_attributed' : 'int8',\n",
    "        'click_id'      : 'int32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/train.csv', dtype=dtypes, parse_dates=['click_time', 'attributed_time'])\n",
    "df.to_parquet('../data/intermed/train.parquet')\n",
    "\n",
    "class0 = df.loc[df.is_attributed == 0, :].sample(frac=.1)\n",
    "class0.to_parquet('../data/intermed/train0_10pct.parquet')\n",
    "\n",
    "class1 = df.loc[df.is_attributed == 1, :]\n",
    "class1.to_parquet('../data/intermed/train1.parquet')\n",
    "\n",
    "# clear biggest file from memory before doing smaller files\n",
    "del df, class0, class1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/train_sample.csv', dtype=dtypes, parse_dates=['click_time', 'attributed_time'])\n",
    "df.to_parquet('../data/intermed/train_sample.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/test_supplement.csv', dtype=dtypes, parse_dates=['click_time'])\n",
    "df.to_parquet('../data/intermed/test_supplement.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/test.csv', dtype=dtypes, parse_dates=['click_time'])\n",
    "df.to_parquet('../data/intermed/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
